{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCE_Jh6b7OjI"
      },
      "source": [
        "Step_no = rI = dA = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzTtmQ1a7Vwo"
      },
      "source": [
        "import cv2, functools, gc, glob, math, numpy as np, os, pandas, random, tensorflow as tf, time, warnings\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.layers import Activation, add, BatchNormalization, concatenate, Conv1D, Conv2D, Conv3D, Dense, dot, Dropout\n",
        "from keras.layers import Flatten, GlobalAveragePooling2D, Input, Lambda, MaxPool1D, MaxPooling2D, multiply, Reshape\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from PIL import Image, ImageEnhance\n",
        "from skimage import io, transform\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "warnings.filterwarnings(\"ignore\", message = \"Numerical issues were encountered\")\n",
        "warnings.filterwarnings(\"ignore\", message = \"Creating an ndarray from ragged nested sequences\")\n",
        "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_z_yAap7VyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04dec061-75e8-47f1-bc72-7b7669db3322"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyRS6trZ7V14",
        "outputId": "edda25de-e362-4631-8d9d-c160de01534a"
      },
      "source": [
        "dir = \"/content/drive/MyDrive/classified data\"\n",
        "doc = open(\"all_files.txt\",'w')\n",
        "k = l = 0\n",
        "for root, dir, files in os.walk(dir):\n",
        "    for file in files:\n",
        "        k += 1\n",
        "        if file.split('.')[-1]  in ['jpg', 'JPG']:\n",
        "            l += 1\n",
        "            print(os.path.join(root,file), file = doc)\n",
        "print(k, l)\n",
        "doc.close()\n",
        "lastLine, classnum = \"\", -1\n",
        "list_file = open(\"file_list.txt\", \"w\")\n",
        "with open(\"all_files.txt\") as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        thisline = line.split(\"/\")[-3]+line.split(\"/\")[-2]\n",
        "        if lastLine != thisline:\n",
        "            classnum, lastLine = classnum + 1, thisline\n",
        "        list_file.write(line.split(\"\\n\")[0]+\"\\t\"+str(classnum)+\"\\n\")\n",
        "        line = f.readline()\n",
        "f.close()\n",
        "list_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoWcWFJw7V3S"
      },
      "source": [
        "def creat_list(path):\n",
        "    lists = [[] for i in range(2)]\n",
        "    with open(path) as f:\n",
        "        line = f.readline()\n",
        "        while line:\n",
        "            classnum = int(line.split(\"\\t\")[1])\n",
        "            if classnum == 0 or classnum == 1 or classnum == 3: # NE or EH or EP\n",
        "                lists[0].append(line.split(\"\\t\")[0])\n",
        "            else: # EA\n",
        "                lists[1].append(line.split(\"\\t\")[0])\n",
        "            line = f.readline()\n",
        "    f.close()\n",
        "    return np.array(lists)\n",
        "\n",
        "list = creat_list(\"file_list.txt\")\n",
        "random_list = open(\"binary_random_list.txt\",\"w\")\n",
        "for c in range(len(list)):\n",
        "    random.shuffle(list[c])\n",
        "    for item in list[c]:\n",
        "        random_list.write(str(item)+\"\\t\"+str(c)+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSsfneJU7V62"
      },
      "source": [
        "def slice_train_test(array, i, K): # called inside cross_validation Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": slice_train_test Function called.\")\n",
        "    Step_no += 1\n",
        "    exp_image_cnt_in_test = len(array) // K\n",
        "    test_start = i * exp_image_cnt_in_test\n",
        "    test_end = test_start + exp_image_cnt_in_test\n",
        "    test_array = array[test_start : test_end]\n",
        "    train_array = array[ : test_start] + array[test_end : ]\n",
        "    return train_array, test_array # The output is the category-wise lists of train-data paths and test-data paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZYh6FCG7V8f"
      },
      "source": [
        "def read_image_from_path_as_array(imagePath, width = 600, height = 600, normalization = True): # called inside cross_validation Function\n",
        "    global Step_no, rI\n",
        "    if rI == 1:\n",
        "        print(str(Step_no) + \": read_image_from_path_as_array Function called.\")\n",
        "    Step_no, rI = Step_no + 1, rI + 1\n",
        "    img = io.imread(imagePath.split('\\n')[0]) # print(img) gives ndarray  # print(width, height)\n",
        "    imageData = transform.resize(img, (width, height, 3))\n",
        "    if normalization == True:\n",
        "        imageData = np.transpose(imageData, (2, 0, 1))\n",
        "        imageData = [preprocessing.scale(imageData[i]) for i in range(3)]\n",
        "        imageData = np.transpose(imageData,(1, 2, 0)) # imageData = transform.resize(img,(width, height, 3))\n",
        "    return imageData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGyYS4qG7V_o"
      },
      "source": [
        "def _convND(ip, rank, channels): # called inside non_local_block Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": _convND Function called.\")\n",
        "    Step_no += 1\n",
        "    assert rank in [3, 4, 5]  # Rank of input must be 3, 4 or 5\n",
        "    if rank == 3:\n",
        "        x = Conv1D(channels, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(ip)\n",
        "    elif rank == 4:\n",
        "        x = Conv2D(channels, (1, 1), padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(ip)\n",
        "    else:\n",
        "        x = Conv3D(channels, (1, 1, 1), padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(ip)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgWItQNi7WBq"
      },
      "source": [
        "def non_local_block(ip, intermediate_dim = None, compression = 2, mode = 'embedded', add_residual = True): # called inside Network_Config Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": non_local_block Function called.\")\n",
        "    Step_no += 1\n",
        "    channel_dim = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "    ip_shape = K.int_shape(ip)\n",
        "    if mode not in ['gaussian', 'embedded', 'dot', 'concatenate']:\n",
        "        raise ValueError('`mode` must be one of `gaussian`, `embedded`, `dot` or `concatenate`')\n",
        "    compression = 1 if compression is None else compression\n",
        "    dim1, dim2, dim3 = None, None, None # check rank to calculate the input shape\n",
        "    if len(ip_shape) == 3:  # temporal / time series data\n",
        "        rank = 3\n",
        "        batchsize, dim1, channels = ip_shape\n",
        "    elif len(ip_shape) == 4:  # spatial / image data\n",
        "        rank = 4\n",
        "        if channel_dim == 1:\n",
        "           batchsize, channels, dim1, dim2 = ip_shape\n",
        "        else:\n",
        "           batchsize, dim1, dim2, channels = ip_shape\n",
        "    elif len(ip_shape) == 5:  # spatio-temporal / Video or Voxel data\n",
        "        rank = 5\n",
        "        if channel_dim == 1:\n",
        "            batchsize, channels, dim1, dim2, dim3 = ip_shape\n",
        "        else:\n",
        "            batchsize, dim1, dim2, dim3, channels = ip_shape\n",
        "    else:\n",
        "        raise ValueError('Input dimension has to be either 3 (temporal), 4 (spatial) or 5 (spatio-temporal)')\n",
        "    if intermediate_dim is None:\n",
        "        intermediate_dim = channels // 2\n",
        "        if intermediate_dim < 1:\n",
        "            intermediate_dim = 1\n",
        "    else:\n",
        "        intermediate_dim = int(intermediate_dim)\n",
        "        if intermediate_dim < 1:\n",
        "            raise ValueError('`intermediate_dim` must be either `None` or positive integer greater than 1.')\n",
        "    # print(\"intermediate_dim \" + str(intermediate_dim))\n",
        "    if mode == 'gaussian':  # Gaussian instantiation\n",
        "        x1 = Reshape((-1, channels))(ip)  # xi\n",
        "        x2 = Reshape((-1, channels))(ip)  # xj\n",
        "        f = dot([x1, x2], axes = 2)\n",
        "        f = Activation('softmax')(f)\n",
        "    elif mode == 'dot':  # Dot instantiation\n",
        "        theta = _convND(ip, rank, intermediate_dim) # theta path\n",
        "        theta = Reshape((-1, intermediate_dim))(theta)\n",
        "        phi = _convND(ip, rank, intermediate_dim) # phi path\n",
        "        phi = Reshape((-1, intermediate_dim))(phi)\n",
        "        f = dot([theta, phi], axes = 2)\n",
        "        size = K.int_shape(f)\n",
        "        f = Lambda(lambda z: (1. / float(size[-1])) * z)(f) # scale the values to make it size invariant\n",
        "    elif mode == 'concatenate':  # Concatenation instantiation\n",
        "        raise NotImplementedError('Concatenate model has not been implemented yet')\n",
        "    else:  # Embedded Gaussian instantiation\n",
        "        # print(\"ip, rank, intermediate_dim: \" + str(ip) + str(rank) + str(intermediate_dim))\n",
        "        theta = _convND(ip, rank, intermediate_dim) # theta path\n",
        "        theta = Reshape((-1, intermediate_dim))(theta)\n",
        "        phi = _convND(ip, rank, intermediate_dim) # phi path\n",
        "        phi = Reshape((-1, intermediate_dim))(phi)\n",
        "        if compression > 1: # shielded computation\n",
        "            phi = MaxPool1D(compression)(phi)\n",
        "        f = dot([theta, phi], axes = 2)\n",
        "        f = Activation('softmax')(f)\n",
        "    g = _convND(ip, rank, intermediate_dim) # g path\n",
        "    g = Reshape((-1, intermediate_dim))(g)\n",
        "    if compression > 1 and mode == 'embedded': # shielded computation\n",
        "        g = MaxPool1D(compression)(g)\n",
        "    y = dot([f, g], axes=[2, 1]) # compute output path\n",
        "    if rank == 3: # reshape to input tensor format starts\n",
        "        y = Reshape((dim1, intermediate_dim))(y)\n",
        "    elif rank == 4:\n",
        "        if channel_dim == -1:\n",
        "            y = Reshape((dim1, dim2, intermediate_dim))(y)\n",
        "        else:\n",
        "            y = Reshape((intermediate_dim, dim1, dim2))(y)\n",
        "    else:\n",
        "        if channel_dim == -1:\n",
        "            y = Reshape((dim1, dim2, dim3, intermediate_dim))(y)\n",
        "        else:\n",
        "            y = Reshape((intermediate_dim, dim1, dim2, dim3))(y)\n",
        "    y = _convND(y, rank, channels) # project filters starts\n",
        "    if add_residual: # residual connection starts\n",
        "        y = concatenate([ip, y], axis = 3)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EChc4tjJ7WEq"
      },
      "source": [
        "def squeeze_excitation_layer(x, out_dim, ratio = 4, concate = True): # called inside Network_Config Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": squeeze_excitation_layer Function called.\")\n",
        "    Step_no += 1\n",
        "    squeeze = GlobalAveragePooling2D()(x)\n",
        "    excitation = Dense(units=out_dim // ratio)(squeeze)\n",
        "    excitation = Activation('relu')(excitation)\n",
        "    excitation = Dense(units = out_dim)(excitation)\n",
        "    excitation = Activation('sigmoid')(excitation)\n",
        "    excitation = Reshape((1, 1, out_dim))(excitation)\n",
        "    scale = multiply([x, excitation])\n",
        "    if concate:\n",
        "        scale = concatenate([scale, x], axis=3)\n",
        "    return scale # inter-channel weighting."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ezWzg7G7WGQ"
      },
      "source": [
        "def create_directory(dir_path): # called inside Network_Config Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": create_directory Function called.\")\n",
        "    Step_no += 1\n",
        "    print(dir_path + \" is created using create_directory Function.\" )\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKhlOA3z7WJY"
      },
      "source": [
        "def dataAugmentation(input_array, flip_left_right_rate = 0.5, flip_top_bottom_rate = 0.5): # called inside batch_generator Function\n",
        "    global Step_no, dA\n",
        "    if dA == 1:\n",
        "        print(str(Step_no) + \": dataAugmentation Function called.\")\n",
        "    Step_no, dA = Step_no + 1, dA + 1\n",
        "    if (random.random() < flip_left_right_rate): # Flip left and right\n",
        "        input_array = np.transpose(input_array, (2, 0, 1)) # w, h, 3 -> 3, w, h\n",
        "        input_array = [np.flip(input_array[i], 1) for i in range(3)]\n",
        "        input_array = np.transpose(input_array, (1, 2, 0)) # 3, w, h -> w, h, 3\n",
        "    if (random.random() < flip_top_bottom_rate): # Flip upside down\n",
        "        input_array = np.transpose(input_array, (2, 0, 1))  # w, h, 3 -> 3, w, h\n",
        "        input_array = [np.flip(input_array[i], 0) for i in range(3)]\n",
        "        input_array = np.transpose(input_array, (1, 2, 0))  # 3, w, h -> w, h, 3\n",
        "    return np.array(input_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjFnpgUY7WMO"
      },
      "source": [
        "def batch_generator(all_data, all_label, batch_size, shuffle, class_num = 4, train = True): # called inside Network_Config Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": batch_generator Function called.\")\n",
        "    Step_no += 1\n",
        "    assert len(all_data) == len(all_label)\n",
        "    print(\"Size of all_data in batch_generator function: \"+ str(len(all_data)))\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(all_data))\n",
        "        random.shuffle(indices)\n",
        "    k = 0\n",
        "    while True:\n",
        "        k += 1\n",
        "        for start_idx in range(0, len(all_data) - batch_size + 1, batch_size):\n",
        "            data, labels = [], []\n",
        "            excerpt = indices[start_idx : start_idx + batch_size] if shuffle else slice(start_idx, start_idx + batch_size)\n",
        "            for di in excerpt:\n",
        "                tmp_data = all_data[di]\n",
        "                if train:\n",
        "                    tmp_data = dataAugmentation(tmp_data)\n",
        "                data.append(tmp_data)\n",
        "            for li in excerpt:\n",
        "                cla, tmp = all_label[li], [0 for x in range(class_num)]\n",
        "                tmp[cla] = 1\n",
        "                labels.append(tmp)\n",
        "            print(len(data))\n",
        "            yield np.array(data), np.array(labels)\n",
        "    global dA\n",
        "    print(\"-------------------------------------------------------------------------dA: \" + str(dA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khHQ0z5V7xHr"
      },
      "source": [
        "def batch_generator_confusion_matrix(all_data, all_label, batch_size, shuffle, class_num = 4): # called inside Network_Config method\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": batch_generator_confusion_matrix Function called.\")\n",
        "    Step_no += 1\n",
        "    assert len(all_data) == len(all_label)\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(all_data))\n",
        "        random.shuffle(indices)\n",
        "    for start_idx in range(0, len(all_data) - batch_size + 1, batch_size):\n",
        "        data, labels = [], []\n",
        "        excerpt = indices[start_idx : start_idx + batch_size] if shuffle else slice(start_idx, start_idx + batch_size)\n",
        "        for di in excerpt:\n",
        "            tmp_data = all_data[di]\n",
        "            data.append(all_data[di])\n",
        "        for li in excerpt:\n",
        "            cla, tmp = all_label[li], [0 for x in range(class_num)]\n",
        "            tmp[cla] = 1\n",
        "            labels.append(tmp)\n",
        "        yield np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2SiGPyQ7xTs"
      },
      "source": [
        "def Network_Config(Test_data, Test_labels, Train_data = None, Train_labels = None, No_of_categories = 0, No_of_epochs = 0, Initial_epoch = 0, Batch_size = 127, Iteration_no = 0): # called inside cross_validation Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": Network_Config Function called.\")\n",
        "    Step_no += 1\n",
        "    adam = Adam(learning_rate = 0.005, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0009)\n",
        "    sgd = SGD(learning_rate = 0.001, momentum = 0.9, decay = 0.0, nesterov = False)\n",
        "    input_tensor = Input(shape = (224, 224, 3))\n",
        "    K.clear_session()\n",
        "    base_model = VGG16(input_tensor = input_tensor, weights = 'imagenet', include_top = False) # backbone\n",
        "    base_output = base_model.output\n",
        "    x = non_local_block(base_output, intermediate_dim = None, compression = 2, mode = 'embedded', add_residual = False)\n",
        "    x = BatchNormalization()(x)\n",
        "    y = squeeze_excitation_layer(base_output, 512, ratio = 4, concate = False) # channel-attention\n",
        "    y = BatchNormalization()(y)\n",
        "    x = concatenate([base_output, x], axis = 3) # concat\n",
        "    x = concatenate([x, y], axis = 3)\n",
        "    gap = GlobalAveragePooling2D()(x) # spp\n",
        "    x = Flatten()(x)\n",
        "    x = concatenate([gap, x])\n",
        "    x = Dense(512, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(512, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(512, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    predict = Dense(No_of_categories, activation = 'softmax')(x)\n",
        "    model = Model(inputs = input_tensor, outputs = predict)\n",
        "    for layer in (base_model.layers):\n",
        "        layer.trainable = False\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = [keras.metrics.categorical_accuracy])\n",
        "    print(\"<--------------------------------------------------Model Summary--------------------------------------------------------->\")\n",
        "    model.summary()\n",
        "    print(\"------------------------------------------------End of model summary------------------------------------------------------\")\n",
        "    create_directory('./final/')\n",
        "    weights_file = './final/' + str(Iteration_no) + '-weights.{epoch:02d}-{categorical_accuracy:.4f}-{val_loss:.4f}-{val_categorical_accuracy:.4f}.h5'\n",
        "    csv_file = './final/record.csv'\n",
        "    lr_reducer = ReduceLROnPlateau(monitor = 'categorical_accuracy', factor = 0.2, cooldown = 0, patience = 2, min_lr = 0.5e-6)\n",
        "    early_stopper = EarlyStopping(monitor = 'val_categorical_accuracy', min_delta = 1e-4, patience = 30)\n",
        "    model_checkpoint = ModelCheckpoint(weights_file, monitor = 'val_categorical_accuracy', save_best_only = True, verbose = 1, save_weights_only = True, mode = 'max')\n",
        "    tensorboard = TensorBoard(log_dir = './logs/', histogram_freq = 0, write_graph = True, write_images = True, embeddings_freq = 0, embeddings_layer_names = None, embeddings_metadata = None)\n",
        "    CSV_record = CSVLogger(csv_file, separator = ',', append = True)\n",
        "    callbacks = [lr_reducer, early_stopper, model_checkpoint, tensorboard, CSV_record]\n",
        "    gc.disable()\n",
        "    print(\"Printing test label: \"+ str(Test_labels)) # max_q_size=0\n",
        "    model.fit_generator(\n",
        "        generator = batch_generator(np.array(Train_data), np.array(Train_labels), Batch_size, True, No_of_categories, True),\n",
        "        steps_per_epoch = int(len(Train_labels)/Batch_size)-1,\n",
        "        max_queue_size = 20,\n",
        "        initial_epoch = Initial_epoch,\n",
        "        epochs = No_of_epochs,\n",
        "        verbose = 1,\n",
        "        callbacks = callbacks,\n",
        "        validation_data = batch_generator(np.array(Test_data), np.array(Test_labels), Batch_size, True, No_of_categories, False),\n",
        "        validation_steps = int( len(Test_labels)/Batch_size )-1 # class_weight=NULL\n",
        "        )\n",
        "    print(\"<--------------------------------------------------Confusion Matrix process starts-------------------------------------------------->\")\n",
        "    Total_predicted_labels, Total_true_labels = [], []\n",
        "    print(\"Batch size: \" + str(Batch_size))\n",
        "    for test_data_batch, test_labels_batch in batch_generator_confusion_matrix(np.array(Test_data), np.array(Test_labels), Batch_size, True, No_of_categories):\n",
        "        print(\"len(test_data_batch): \"+str(len(test_data_batch)))\n",
        "        y_pred = model.predict(test_data_batch, Batch_size)\n",
        "        y_true = test_labels_batch\n",
        "        for y_p in y_pred:\n",
        "            Total_predicted_labels.append(np.where(y_p == max(y_p))[0][0])\n",
        "        for y_t in y_true:\n",
        "            Total_true_labels.append(np.where(y_t == max(y_t))[0][0])\n",
        "    confusion = confusion_matrix(y_true = Total_true_labels, y_pred = Total_predicted_labels)\n",
        "    print(\"Total_predicted_labels: \" + str(len(Total_predicted_labels)) )\n",
        "    print(\"----------------------------------Printing confusion matrix for the iteration - \" + str(Iteration_no) + \"---------------------------------\")\n",
        "    print(confusion)\n",
        "    f = open('confusion_matrix.txt','a+')\n",
        "    f.write(str(Total_true_labels)+\"\\n\")\n",
        "    f.write(str(Total_predicted_labels)+\"\\n\")\n",
        "    f.write(str(confusion)+'\\n')\n",
        "    f.close()\n",
        "    gc.enable()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTKqpkzt73KM"
      },
      "source": [
        "def cross_validation(categorised_data, K, no_of_epochs, no_of_classes, batch_size): # called inside main Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": cross_validation Function called.\")\n",
        "    Step_no += 1\n",
        "    no_of_categories = len(categorised_data) # categorised_data = array of size 4, each index has an array of NE, EP, EH, EA\n",
        "    for each_category in range(no_of_categories):\n",
        "        random.shuffle(categorised_data[each_category])\n",
        "    print(\"<--------------------------------------------------Iterations initiated-------------------------------------------------->\")\n",
        "    for iteration in range(K):\n",
        "        print(\"Iteration no: %d\" %iteration)\n",
        "        train_data_for_present_iteration, test_data_for_present_iteration, train_data_paths, train_data_labels, test_data_paths, test_data_labels = [], [], [], [], [], []\n",
        "        for category_no in range(no_of_categories):\n",
        "            train_data_paths_of_each_cat, test_data_paths_of_each_cat = slice_train_test(categorised_data[category_no], iteration, K)\n",
        "            print(\"Category-wise train-size: \" + str(len(train_data_paths_of_each_cat))+ \", Category-wise test-size: \" + str(len(test_data_paths_of_each_cat)))\n",
        "            for each_path in range(len(train_data_paths_of_each_cat)):\n",
        "                train_data_paths.append(train_data_paths_of_each_cat[each_path])\n",
        "                train_data_labels.append(category_no)\n",
        "            for each_path in range(len(test_data_paths_of_each_cat)):\n",
        "                test_data_paths.append(test_data_paths_of_each_cat[each_path])\n",
        "                test_data_labels.append(category_no)\n",
        "        print(\"Size of training dataset: \" + str(len(train_data_paths)) + \", no. of training labels: \" + str(len(train_data_labels)))\n",
        "        print(\"Size of testing dataset: \" + str(len(test_data_paths)) + \", no. of testing labels: \" + str(len(test_data_labels)))\n",
        "        record = open(\"record_of_iterations.txt\", 'a+')\n",
        "        record.write(\"Iteration no: \" + str(iteration))\n",
        "        record.write(str(train_data_paths)+'\\n')\n",
        "        record.write(str(test_data_paths)+'\\n')\n",
        "        record.close()\n",
        "        for each_path in train_data_paths:\n",
        "            train_data_for_present_iteration.append(read_image_from_path_as_array(each_path, 224, 224, True))\n",
        "        for each_path in test_data_paths:\n",
        "            test_data_for_present_iteration.append(read_image_from_path_as_array(each_path, 224, 224, True))\n",
        "        Network_Config(Train_data = train_data_for_present_iteration, No_of_categories = no_of_categories, No_of_epochs = no_of_epochs, Train_labels = train_data_labels,\n",
        "                       Test_data = test_data_for_present_iteration, Test_labels = test_data_labels, Iteration_no = iteration, Initial_epoch = 0, Batch_size = batch_size)\n",
        "        print(rI)\n",
        "        print(dA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQSMPNPS75Yw",
        "outputId": "712b67ed-9042-4421-ba8e-da005de7777f"
      },
      "source": [
        "def main():\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": main Function called.\")\n",
        "    Step_no += 1\n",
        "    list = creat_list(\"/content/binary_random_list.txt\")\n",
        "    cross_validation(categorised_data = list, K = 10, no_of_epochs = 12, no_of_classes = 2, batch_size = 32)\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "767: main Function called.\n",
            "768: cross_validation Function called.\n",
            "<--------------------------------------------------Iterations initiated-------------------------------------------------->\n",
            "Iteration no: 0\n",
            "769: slice_train_test Function called.\n",
            "Category-wise train-size: 265, Category-wise test-size: 29\n",
            "770: slice_train_test Function called.\n",
            "Category-wise train-size: 0, Category-wise test-size: 0\n",
            "Size of training dataset: 265, no. of training labels: 265\n",
            "Size of testing dataset: 29, no. of testing labels: 29\n",
            "1065: Network_Config Function called.\n",
            "1066: non_local_block Function called.\n",
            "1067: _convND Function called.\n",
            "1068: _convND Function called.\n",
            "1069: _convND Function called.\n",
            "1070: _convND Function called.\n",
            "1071: squeeze_excitation_layer Function called.\n",
            "<--------------------------------------------------Model Summary--------------------------------------------------------->\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 7, 7, 256)    131072      block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 7, 7, 256)    131072      block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 49, 256)      0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 49, 256)      0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 24, 256)      0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 7, 7, 256)    131072      block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 512)          0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 49, 24)       0           reshape[0][0]                    \n",
            "                                                                 max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 49, 256)      0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          65664       global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 49, 24)       0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 24, 256)      0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 128)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 49, 256)      0           activation[0][0]                 \n",
            "                                                                 max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          66048       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 7, 7, 256)    0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 7, 7, 512)    131072      reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 1, 1, 512)    0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 7, 7, 512)    2048        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 7, 7, 512)    0           block5_pool[0][0]                \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 7, 7, 1024)   0           block5_pool[0][0]                \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 7, 7, 512)    2048        multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 7, 7, 1536)   0           concatenate[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 1536)         0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 75264)        0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 76800)        0           global_average_pooling2d_1[0][0] \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          39322112    concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512)          2048        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 512)          262656      batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512)          2048        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          262656      batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 512)          2048        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 2)            1026        batch_normalization_4[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 55,229,378\n",
            "Trainable params: 40,509,570\n",
            "Non-trainable params: 14,719,808\n",
            "__________________________________________________________________________________________________\n",
            "------------------------------------------------End of model summary------------------------------------------------------\n",
            "1072: create_directory Function called.\n",
            "./final/ is created using create_directory Function.\n",
            "Printing test label: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "1073: batch_generator Function called.\n",
            "Size of all_data in batch_generator function: 265\n",
            "32\n",
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "32\n",
            "32\n",
            "1/7 [===>..........................] - ETA: 19s - loss: 0.9971 - categorical_accuracy: 0.593832\n",
            "32\n",
            "2/7 [=======>......................] - ETA: 1s - loss: 1.2325 - categorical_accuracy: 0.5938 32\n",
            "32\n",
            "3/7 [===========>..................] - ETA: 1s - loss: 1.7258 - categorical_accuracy: 0.583332\n",
            "32\n",
            "4/7 [================>.............] - ETA: 0s - loss: 1.9683 - categorical_accuracy: 0.582032\n",
            "32\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 2.0958 - categorical_accuracy: 0.584432\n",
            "32\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 2.1441 - categorical_accuracy: 0.585132\n",
            "32\n",
            "7/7 [==============================] - ETA: 0s - loss: 2.1544 - categorical_accuracy: 0.58631330: batch_generator Function called.\n",
            "Size of all_data in batch_generator function: 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldcoSKmU84s7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}