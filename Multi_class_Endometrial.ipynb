{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mepLB4YbJTzj"
      },
      "outputs": [],
      "source": [
        "Step_no = rI = dA = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioVFjZaxLO8e"
      },
      "outputs": [],
      "source": [
        "import cv2, functools, gc, glob, math, numpy as np, os, pandas, random, tensorflow as tf, time, warnings\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.layers import Activation, add, BatchNormalization, concatenate, Conv1D, Conv2D, Conv3D, Dense, dot, Dropout\n",
        "from keras.layers import Flatten, GlobalAveragePooling2D, Input, Lambda, MaxPool1D, MaxPooling2D, multiply, Reshape\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from PIL import Image, ImageEnhance\n",
        "from skimage import io, transform\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "warnings.filterwarnings(\"ignore\", message = \"Numerical issues were encountered\")\n",
        "warnings.filterwarnings(\"ignore\", message = \"Creating an ndarray from ragged nested sequences\")\n",
        "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDI9-RcSSlwA",
        "outputId": "8a414f40-1ac3-4d91-f0b7-a036c5c7ea7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9x7ufo5mxiz",
        "outputId": "243348fc-2bc4-4944-f0d2-af8cb9f25971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3302 3302\n"
          ]
        }
      ],
      "source": [
        "dir = \"/content/drive/MyDrive/DATASET\"\n",
        "doc, k, l, lastLine, classnum = open(\"all_files.txt\",'w'), 0, 0, \"\", -1\n",
        "for root, dir, files in os.walk(dir):\n",
        "    for file in files:\n",
        "        k += 1\n",
        "        if file.split('.')[-1]  in ['jpg', 'JPG']:\n",
        "            l += 1\n",
        "            print(os.path.join(root,file), file = doc)\n",
        "print(k, l)\n",
        "doc.close()\n",
        "list_file = open(\"file_list.txt\",\"w\")\n",
        "with open(\"all_files.txt\") as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        thisline = line.split(\"/\")[-3]+line.split(\"/\")[-2]\n",
        "        if lastLine != thisline:\n",
        "            classnum, lastLine = classnum + 1, thisline\n",
        "        list_file.write(line.split(\"\\n\")[0]+\"\\t\"+str(classnum)+\"\\n\")\n",
        "        line = f.readline()\n",
        "f.close()\n",
        "list_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB1mau8bfVV4"
      },
      "outputs": [],
      "source": [
        "def creat_list(path): # called inside main Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": creat_list Function called.\")\n",
        "    Step_no, lists = Step_no + 1, [[] for i in range(4)]\n",
        "    with open(path) as f:\n",
        "        line, k = f.readline(), 0\n",
        "        while line:\n",
        "            classnum = int(line.split(\"\\t\")[1]) # classnum: 1 for NE, 3 for EP, 2 for EH, 0 for EA\n",
        "            lists_index = 0 if classnum == 1 else 1 if classnum == 3 else 2 if classnum == 2 else 3\n",
        "            lists[lists_index].append(line.split(\"\\t\")[0])\n",
        "            line, k = f.readline(), k + 1\n",
        "    l = {\"NE\": len(lists[0]), \"EP\": len(lists[1]), \"EH\": len(lists[2]), \"EA\": len(lists[3])}\n",
        "    print(k, l)\n",
        "    f.close()\n",
        "    return np.array(lists, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxk4BsYvqmmK"
      },
      "outputs": [],
      "source": [
        "def slice_train_test(array, i, K): # called inside cross_validation Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": slice_train_test Function called.\")\n",
        "    Step_no += 1\n",
        "    exp_image_cnt_in_test = len(array) // K\n",
        "    test_start = i * exp_image_cnt_in_test\n",
        "    test_end = test_start + exp_image_cnt_in_test\n",
        "    test_array = array[test_start : test_end]\n",
        "    train_array = array[ : test_start] + array[test_end : ]\n",
        "    return train_array, test_array # The output is the category-wise lists of train-data paths and test-data paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukBY9ZIWoQ_C"
      },
      "outputs": [],
      "source": [
        "def read_image_from_path_as_array(imagePath, width = 600, height = 600, normalization = True): # called inside cross_validation Function\n",
        "    global Step_no, rI\n",
        "    if rI == 1:\n",
        "        print(str(Step_no) + \": read_image_from_path_as_array Function called.\")\n",
        "    Step_no, rI = Step_no + 1, rI + 1\n",
        "    img = io.imread(imagePath.split('\\n')[0]) # print(img) gives ndarray  # print(width, height)\n",
        "    imageData = transform.resize(img, (width, height, 3))\n",
        "    if normalization == True:\n",
        "        imageData = np.transpose(imageData, (2, 0, 1))\n",
        "        imageData = [preprocessing.scale(imageData[i]) for i in range(3)]\n",
        "        imageData = np.transpose(imageData,(1, 2, 0)) # imageData = transform.resize(img,(width, height, 3))\n",
        "    return imageData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n0DfjEQ5F9l"
      },
      "outputs": [],
      "source": [
        "def _convND(ip, rank, channels): # called inside non_local_block Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": _convND Function called.\")\n",
        "    Step_no += 1\n",
        "    assert rank in [3, 4, 5]  # Rank of input must be 3, 4 or 5\n",
        "    if rank == 3:\n",
        "        x = Conv1D(channels, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(ip)\n",
        "    elif rank == 4:\n",
        "        x = Conv2D(channels, (1, 1), padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(ip)\n",
        "    else:\n",
        "        x = Conv3D(channels, (1, 1, 1), padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(ip)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_5f3O7c0w3w"
      },
      "outputs": [],
      "source": [
        "def non_local_block(ip, intermediate_dim = None, compression = 2, mode = 'embedded', add_residual = True): # called inside Network_Config Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": non_local_block Function called.\")\n",
        "    Step_no += 1\n",
        "    channel_dim = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "    print(\"K.image_data_format(): \"+str(K.image_data_format()))\n",
        "    ip_shape = K.int_shape(ip)\n",
        "    if mode not in ['gaussian', 'embedded', 'dot', 'concatenate']:\n",
        "        raise ValueError('`mode` must be one of `gaussian`, `embedded`, `dot` or `concatenate`')\n",
        "    compression = 1 if compression is None else compression\n",
        "    dim1, dim2, dim3 = None, None, None # check rank to calculate the input shape\n",
        "    if len(ip_shape) == 3:  # temporal / time series data\n",
        "        rank = 3\n",
        "        batchsize, dim1, channels = ip_shape\n",
        "    elif len(ip_shape) == 4:  # spatial / image data\n",
        "        rank = 4\n",
        "        if channel_dim == 1:\n",
        "           batchsize, channels, dim1, dim2 = ip_shape\n",
        "        else:\n",
        "           batchsize, dim1, dim2, channels = ip_shape\n",
        "    elif len(ip_shape) == 5:  # spatio-temporal / Video or Voxel data\n",
        "        rank = 5\n",
        "        if channel_dim == 1:\n",
        "            batchsize, channels, dim1, dim2, dim3 = ip_shape\n",
        "        else:\n",
        "            batchsize, dim1, dim2, dim3, channels = ip_shape\n",
        "    else:\n",
        "        raise ValueError('Input dimension has to be either 3 (temporal), 4 (spatial) or 5 (spatio-temporal)')\n",
        "    if intermediate_dim is None:\n",
        "        intermediate_dim = channels // 2\n",
        "        if intermediate_dim < 1:\n",
        "            intermediate_dim = 1\n",
        "    else:\n",
        "        intermediate_dim = int(intermediate_dim)\n",
        "        if intermediate_dim < 1:\n",
        "            raise ValueError('`intermediate_dim` must be either `None` or positive integer greater than 1.')\n",
        "    if mode == 'gaussian':  # Gaussian instantiation\n",
        "        x1 = Reshape((-1, channels))(ip)  # xi\n",
        "        x2 = Reshape((-1, channels))(ip)  # xj\n",
        "        f = dot([x1, x2], axes = 2)\n",
        "        f = Activation('softmax')(f)\n",
        "    elif mode == 'dot':  # Dot instantiation\n",
        "        theta = _convND(ip, rank, intermediate_dim) # theta path\n",
        "        theta = Reshape((-1, intermediate_dim))(theta)\n",
        "        phi = _convND(ip, rank, intermediate_dim) # phi path\n",
        "        phi = Reshape((-1, intermediate_dim))(phi)\n",
        "        f = dot([theta, phi], axes = 2)\n",
        "        size = K.int_shape(f)\n",
        "        f = Lambda(lambda z: (1. / float(size[-1])) * z)(f) # scale the values to make it size invariant\n",
        "    elif mode == 'concatenate':  # Concatenation instantiation\n",
        "        raise NotImplementedError('Concatenate model has not been implemented yet')\n",
        "    else:  # Embedded Gaussian instantiation\n",
        "        # print(\"ip, rank, intermediate_dim: \" + str(ip) + str(rank) + str(intermediate_dim))\n",
        "        theta = _convND(ip, rank, intermediate_dim) # theta path\n",
        "        theta = Reshape((-1, intermediate_dim))(theta)\n",
        "        phi = _convND(ip, rank, intermediate_dim) # phi path\n",
        "        phi = Reshape((-1, intermediate_dim))(phi)\n",
        "        if compression > 1: # shielded computation\n",
        "            phi = MaxPool1D(compression)(phi)\n",
        "        f = dot([theta, phi], axes = 2)\n",
        "        f = Activation('softmax')(f)\n",
        "    g = _convND(ip, rank, intermediate_dim) # g path\n",
        "    g = Reshape((-1, intermediate_dim))(g)\n",
        "    if compression > 1 and mode == 'embedded': # shielded computation\n",
        "        g = MaxPool1D(compression)(g)\n",
        "    y = dot([f, g], axes=[2, 1]) # compute output path\n",
        "    if rank == 3: # reshape to input tensor format starts\n",
        "        y = Reshape((dim1, intermediate_dim))(y)\n",
        "    elif rank == 4:\n",
        "        if channel_dim == -1:\n",
        "            y = Reshape((dim1, dim2, intermediate_dim))(y)\n",
        "        else:\n",
        "            y = Reshape((intermediate_dim, dim1, dim2))(y)\n",
        "    else:\n",
        "        if channel_dim == -1:\n",
        "            y = Reshape((dim1, dim2, dim3, intermediate_dim))(y)\n",
        "        else:\n",
        "            y = Reshape((intermediate_dim, dim1, dim2, dim3))(y)\n",
        "    y = _convND(y, rank, channels) # project filters starts\n",
        "    if add_residual: # residual connection starts\n",
        "        y = concatenate([ip, y], axis = 3)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-fVySng5U8c"
      },
      "outputs": [],
      "source": [
        "def squeeze_excitation_layer(x, out_dim, ratio = 4, concate = True): # called inside Network_Config Function # inter-channel weighting.\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": squeeze_excitation_layer Function called.\")\n",
        "    Step_no += 1\n",
        "    squeeze = GlobalAveragePooling2D()(x)\n",
        "    excitation = Dense(units=out_dim // ratio)(squeeze)\n",
        "    excitation = Activation('relu')(excitation)\n",
        "    excitation = Dense(units=out_dim)(excitation)\n",
        "    excitation = Activation('sigmoid')(excitation)\n",
        "    excitation = Reshape((1, 1, out_dim))(excitation)\n",
        "    scale = multiply([x, excitation])\n",
        "    if concate:\n",
        "        scale = concatenate([scale, x], axis=3)\n",
        "    return scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6153jJgSxWPg"
      },
      "outputs": [],
      "source": [
        "def create_directory(dir_path): # called inside Network_Config Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": create_directory Function called.\")\n",
        "    Step_no += 1\n",
        "    print(dir_path + \" is created using create_directory Function.\" )\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cjmli3Tpzee"
      },
      "outputs": [],
      "source": [
        "def dataAugmentation(input_array, flip_left_right_rate = 0.5, flip_top_bottom_rate = 0.5): # called inside batch_generator Function\n",
        "    global Step_no, dA\n",
        "    if dA == 1:\n",
        "        print(str(Step_no) + \": dataAugmentation Function called.\")\n",
        "    Step_no, dA = Step_no + 1, dA + 1\n",
        "    if (random.random() < flip_left_right_rate): # Flip left and right\n",
        "        input_array = np.transpose(input_array, (2, 0, 1)) # w, h, 3 -> 3, w, h\n",
        "        input_array = [np.flip(input_array[i], 1) for i in range(3)]\n",
        "        input_array = np.transpose(input_array, (1, 2, 0)) # 3, w, h -> w, h, 3\n",
        "    if (random.random() < flip_top_bottom_rate): # Flip upside down\n",
        "        input_array = np.transpose(input_array, (2, 0, 1))  # w, h, 3 -> 3, w, h\n",
        "        input_array = [np.flip(input_array[i], 0) for i in range(3)]\n",
        "        input_array = np.transpose(input_array, (1, 2, 0))  # 3, w, h -> w, h, 3\n",
        "    return np.array(input_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr9PI8bQs8R7"
      },
      "outputs": [],
      "source": [
        "def batch_generator(all_data, all_label, batch_size, shuffle, class_num = 4, train = True): # called inside Network_Config Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": batch_generator Function called.\")\n",
        "    Step_no += 1\n",
        "    assert len(all_data) == len(all_label)\n",
        "    print(\"Size of all_data in batch_generator function: \"+ str(len(all_data)))\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(all_data))\n",
        "        random.shuffle(indices)\n",
        "    while True:\n",
        "        for start_idx in range(0, len(all_data) - batch_size + 1, batch_size):\n",
        "            data, labels = [], []\n",
        "            excerpt = indices[start_idx : start_idx + batch_size] if shuffle else slice(start_idx, start_idx + batch_size)\n",
        "            for di in excerpt:\n",
        "                tmp_data = all_data[di]\n",
        "                if train:\n",
        "                    tmp_data = dataAugmentation(tmp_data)\n",
        "                data.append(tmp_data)\n",
        "            for li in excerpt:\n",
        "                cla, tmp = all_label[li], [0 for x in range(class_num)]\n",
        "                tmp[cla] = 1\n",
        "                labels.append(tmp)\n",
        "            yield np.array(data), np.array(labels)\n",
        "    print(\"-------------------------------------------------------------------------dA: \" + str(dA))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS5vWpitwqaQ"
      },
      "outputs": [],
      "source": [
        "def batch_generator_confusion_matrix(all_data, all_label, batch_size, shuffle, class_num = 4): # called inside Network_Config method\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": batch_generator_confusion_matrix Function called.\")\n",
        "    Step_no += 1\n",
        "    assert len(all_data) == len(all_label)\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(all_data))\n",
        "        random.shuffle(indices)\n",
        "    for start_idx in range(0, len(all_data) - batch_size + 1, batch_size):\n",
        "        data, labels = [], []\n",
        "        excerpt = indices[start_idx : start_idx + batch_size] if shuffle else slice(start_idx, start_idx + batch_size)\n",
        "        for di in excerpt:\n",
        "            tmp_data = all_data[di]\n",
        "            data.append(all_data[di])\n",
        "        for li in excerpt:\n",
        "            cla, tmp = all_label[li], [0 for x in range(class_num)]\n",
        "            tmp[cla] = 1\n",
        "            labels.append(tmp)\n",
        "        yield np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbp9iABF73Sw"
      },
      "outputs": [],
      "source": [
        "def Network_Config(Test_data, Test_labels, Train_data = None, Train_labels = None, No_of_categories = 0, No_of_epochs = 0, Initial_epoch = 0, Batch_size = 127, Iteration_no = 0): # called inside cross_validation Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": Network_Config Function called.\")\n",
        "    Step_no += 1\n",
        "    adam = Adam(learning_rate = 0.005, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0009)\n",
        "    sgd = SGD(learning_rate = 0.001, momentum = 0.9, decay = 0.0, nesterov = False)\n",
        "    input_tensor = Input(shape = (224, 224, 3))\n",
        "    K.clear_session()\n",
        "    base_model = VGG16(input_tensor = input_tensor, weights = 'imagenet', include_top = False)\n",
        "    base_output = base_model.output\n",
        "    x = non_local_block(base_output, intermediate_dim = None, compression = 2, mode = 'embedded', add_residual = False)\n",
        "    x = BatchNormalization()(x)\n",
        "    y = squeeze_excitation_layer(base_output, 512, ratio = 4, concate = False)\n",
        "    y = BatchNormalization()(y)\n",
        "    x = concatenate([base_output, x], axis = 3)\n",
        "    x = concatenate([x, y], axis = 3)\n",
        "    gap = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = concatenate([gap, x])\n",
        "    x = Dense(512, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(512, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(512, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    predict = Dense(No_of_categories, activation = 'softmax')(x)\n",
        "    model = Model(inputs = input_tensor, outputs = predict)\n",
        "    for layer in (base_model.layers):\n",
        "        layer.trainable = False\n",
        "    for l in model.layers:\n",
        "        print(l.name)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = [keras.metrics.categorical_accuracy])\n",
        "    print(\"<--------------------------------------------------Model Summary--------------------------------------------------------->\")\n",
        "    model.summary()\n",
        "    print(\"------------------------------------------------End of model summary------------------------------------------------------\")\n",
        "    create_directory('./final/')\n",
        "    weights_file = './final/' + str(Iteration_no) + '-weights.{epoch:02d}-{categorical_accuracy:.4f}-{val_loss:.4f}-{val_categorical_accuracy:.4f}.h5'\n",
        "    csv_file = './final/record.csv'\n",
        "    lr_reducer = ReduceLROnPlateau(monitor = 'categorical_accuracy', factor = 0.2, cooldown = 0, patience = 2, min_lr = 0.5e-6)\n",
        "    early_stopper = EarlyStopping(monitor = 'val_categorical_accuracy', min_delta = 1e-4, patience = 30)\n",
        "    model_checkpoint = ModelCheckpoint(weights_file, monitor = 'val_categorical_accuracy', save_best_only = True, verbose = 1, save_weights_only = True, mode = 'max')\n",
        "    tensorboard = TensorBoard(log_dir = './logs/', histogram_freq = 0, write_graph = True, write_images = True, embeddings_freq = 0, embeddings_layer_names = None,\n",
        "                              embeddings_metadata = None)\n",
        "    CSV_record = CSVLogger(csv_file, separator = ',', append = True)\n",
        "    callbacks = [lr_reducer, early_stopper, model_checkpoint, tensorboard, CSV_record]\n",
        "    gc.disable()\n",
        "    print(\"Printing test label: \"+ str(Test_labels))\n",
        "    model.fit_generator(\n",
        "        generator = batch_generator(np.array(Train_data), np.array(Train_labels), Batch_size, True, No_of_categories, True),\n",
        "        steps_per_epoch = int(len(Train_labels)/Batch_size)-1,\n",
        "        max_queue_size = 20,\n",
        "        initial_epoch = Initial_epoch,\n",
        "        epochs = No_of_epochs,\n",
        "        verbose = 1,\n",
        "        callbacks = callbacks,\n",
        "        validation_data = batch_generator(np.array(Test_data), np.array(Test_labels), Batch_size, True, No_of_categories, False),\n",
        "        validation_steps = int( len(Test_labels)/Batch_size )-1 # class_weight=NULL\n",
        "        )\n",
        "    print(\"<--------------------------------------------------Confusion Matrix process starts-------------------------------------------------->\")\n",
        "    Total_predicted_labels, Total_true_labels = [], []\n",
        "    print(\"Batch size: \" + str(Batch_size))\n",
        "    for test_data_batch, test_labels_batch in batch_generator_confusion_matrix(np.array(Test_data), np.array(Test_labels), Batch_size, True, No_of_categories):\n",
        "        print(\"len(test_data_batch): \"+str(len(test_data_batch)))\n",
        "        y_pred = model.predict(test_data_batch, Batch_size)\n",
        "        y_true = test_labels_batch\n",
        "        for y_p in y_pred:\n",
        "            Total_predicted_labels.append(np.where(y_p == max(y_p))[0][0])\n",
        "        for y_t in y_true:\n",
        "            Total_true_labels.append(np.where(y_t == max(y_t))[0][0])\n",
        "    confusion = confusion_matrix(y_true = Total_true_labels, y_pred = Total_predicted_labels)\n",
        "    print(\"Total_predicted_labels: \" + str(len(Total_predicted_labels)) )\n",
        "    print(\"----------------------------------Printing confusion matrix for the iteration - \" + str(Iteration_no) + \"---------------------------------\")\n",
        "    print(confusion)\n",
        "    f = open('confusion_matrix.txt','a+')\n",
        "    f.write(str(Total_true_labels)+\"\\n\")\n",
        "    f.write(str(Total_predicted_labels)+\"\\n\")\n",
        "    f.write(str(confusion)+'\\n')\n",
        "    f.close()\n",
        "    gc.enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPtZCMeaf4YD"
      },
      "outputs": [],
      "source": [
        "def cross_validation(categorised_data, K, no_of_epochs, no_of_classes, batch_size): # called inside main Function\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": cross_validation Function called.\")\n",
        "    Step_no += 1\n",
        "    no_of_categories = len(categorised_data) # categorised_data = array of size 4, each index has an array of NE, EP, EH, EA\n",
        "    for each_category in range(no_of_categories):\n",
        "        random.shuffle(categorised_data[each_category])\n",
        "    print(\"<--------------------------------------------------Iterations initiated-------------------------------------------------->\")\n",
        "    for iteration in range(K):\n",
        "        print(\"Iteration no: %d\" %iteration)\n",
        "        train_data_for_present_iteration, test_data_for_present_iteration, train_data_paths, train_data_labels, test_data_paths, test_data_labels = [], [], [], [], [], []\n",
        "        for category_no in range(no_of_categories):\n",
        "            train_data_paths_of_each_cat, test_data_paths_of_each_cat = slice_train_test(categorised_data[category_no], iteration, K)\n",
        "            print(\"Category-wise train-size: \" + str(len(train_data_paths_of_each_cat))+ \", Category-wise test-size: \" + str(len(test_data_paths_of_each_cat)))\n",
        "            for each_path in range(len(train_data_paths_of_each_cat)):\n",
        "                train_data_paths.append(train_data_paths_of_each_cat[each_path])\n",
        "                train_data_labels.append(category_no)\n",
        "            for each_path in range(len(test_data_paths_of_each_cat)):\n",
        "                test_data_paths.append(test_data_paths_of_each_cat[each_path])\n",
        "                test_data_labels.append(category_no)\n",
        "        print(\"Size of training dataset: \" + str(len(train_data_paths)) + \", no. of training labels: \" + str(len(train_data_labels)))\n",
        "        print(\"Size of testing dataset: \" + str(len(test_data_paths)) + \", no. of testing labels: \" + str(len(test_data_labels)))\n",
        "        record = open(\"record_of_iterations.txt\", 'a+')\n",
        "        record.write(\"Iteration no: \" + str(iteration))\n",
        "        record.write(str(train_data_paths)+'\\n')\n",
        "        record.write(str(test_data_paths)+'\\n')\n",
        "        record.close()\n",
        "        for each_path in train_data_paths:\n",
        "            train_data_for_present_iteration.append(read_image_from_path_as_array(each_path, 224, 224, True))\n",
        "        for each_path in test_data_paths:\n",
        "            test_data_for_present_iteration.append(read_image_from_path_as_array(each_path, 224, 224, True))\n",
        "        Network_Config(Train_data = train_data_for_present_iteration, No_of_categories = no_of_categories, No_of_epochs = no_of_epochs, Train_labels = train_data_labels,\n",
        "                       Test_data = test_data_for_present_iteration, Test_labels = test_data_labels, Iteration_no = iteration, Initial_epoch = 0, Batch_size = batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEIX9PNo-UvF",
        "outputId": "c7616f41-0763-4b16-c106-dfe9708ce165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: main Function called.\n",
            "2: creat_list Function called.\n",
            "3302 {'NE': 1333, 'EP': 636, 'EH': 798, 'EA': 535}\n",
            "3: cross_validation Function called.\n",
            "<--------------------------------------------------Iterations initiated-------------------------------------------------->\n",
            "Iteration no: 0\n",
            "4: slice_train_test Function called.\n",
            "Category-wise train-size: 1200, Category-wise test-size: 133\n",
            "5: slice_train_test Function called.\n",
            "Category-wise train-size: 573, Category-wise test-size: 63\n",
            "6: slice_train_test Function called.\n",
            "Category-wise train-size: 719, Category-wise test-size: 79\n",
            "7: slice_train_test Function called.\n",
            "Category-wise train-size: 482, Category-wise test-size: 53\n",
            "Size of training dataset: 2974, no. of training labels: 2974\n",
            "Size of testing dataset: 328, no. of testing labels: 328\n",
            "8: read_image_from_path_as_array Function called.\n",
            "3310: Network_Config Function called.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "58900480/58889256 [==============================] - 2s 0us/step\n",
            "3311: non_local_block Function called.\n",
            "K.image_data_format(): channels_last\n",
            "3312: _convND Function called.\n",
            "3313: _convND Function called.\n",
            "3314: _convND Function called.\n",
            "3315: _convND Function called.\n",
            "3316: squeeze_excitation_layer Function called.\n",
            "input_1\n",
            "block1_conv1\n",
            "block1_conv2\n",
            "block1_pool\n",
            "block2_conv1\n",
            "block2_conv2\n",
            "block2_pool\n",
            "block3_conv1\n",
            "block3_conv2\n",
            "block3_conv3\n",
            "block3_pool\n",
            "block4_conv1\n",
            "block4_conv2\n",
            "block4_conv3\n",
            "block4_pool\n",
            "block5_conv1\n",
            "block5_conv2\n",
            "block5_conv3\n",
            "block5_pool\n",
            "conv2d_1\n",
            "conv2d\n",
            "reshape_1\n",
            "reshape\n",
            "max_pooling1d\n",
            "conv2d_2\n",
            "global_average_pooling2d\n",
            "dot\n",
            "reshape_2\n",
            "dense\n",
            "activation\n",
            "max_pooling1d_1\n",
            "activation_1\n",
            "dot_1\n",
            "dense_1\n",
            "reshape_3\n",
            "activation_2\n",
            "conv2d_3\n",
            "reshape_4\n",
            "batch_normalization\n",
            "multiply\n",
            "concatenate\n",
            "batch_normalization_1\n",
            "concatenate_1\n",
            "global_average_pooling2d_1\n",
            "flatten\n",
            "concatenate_2\n",
            "dense_2\n",
            "batch_normalization_2\n",
            "dense_3\n",
            "batch_normalization_3\n",
            "dense_4\n",
            "batch_normalization_4\n",
            "dense_5\n",
            "<--------------------------------------------------Model Summary--------------------------------------------------------->\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block5_conv2 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv3 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block5_pool (MaxPooling2D)     (None, 7, 7, 512)    0           ['block5_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 7, 7, 256)    131072      ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 7, 7, 256)    131072      ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 49, 256)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 49, 256)      0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 24, 256)      0           ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 7, 7, 256)    131072      ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 512)         0           ['block5_pool[0][0]']            \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 49, 24)       0           ['reshape[0][0]',                \n",
            "                                                                  'max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 49, 256)      0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          65664       ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 49, 24)       0           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 24, 256)     0           ['reshape_2[0][0]']              \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dot_1 (Dot)                    (None, 49, 256)      0           ['activation[0][0]',             \n",
            "                                                                  'max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 512)          66048       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)            (None, 7, 7, 256)    0           ['dot_1[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 512)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 7, 7, 512)    131072      ['reshape_3[0][0]']              \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)            (None, 1, 1, 512)    0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 7, 7, 512)   2048        ['conv2d_3[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 7, 7, 512)    0           ['block5_pool[0][0]',            \n",
            "                                                                  'reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 7, 7, 1024)   0           ['block5_pool[0][0]',            \n",
            "                                                                  'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 7, 7, 512)   2048        ['multiply[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 7, 7, 1536)   0           ['concatenate[0][0]',            \n",
            "                                                                  'batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 1536)        0           ['concatenate_1[0][0]']          \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 75264)        0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 76800)        0           ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          39322112    ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 512)         2048        ['dense_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 512)          262656      ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 512)         2048        ['dense_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 512)          262656      ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 512)         2048        ['dense_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 4)            2052        ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 55,230,404\n",
            "Trainable params: 40,510,596\n",
            "Non-trainable params: 14,719,808\n",
            "__________________________________________________________________________________________________\n",
            "------------------------------------------------End of model summary------------------------------------------------------\n",
            "3317: create_directory Function called.\n",
            "./final/ is created using create_directory Function.\n",
            "Printing test label: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3318: batch_generator Function called.\n",
            "Size of all_data in batch_generator function: 2974\n",
            "3319: dataAugmentation Function called.\n",
            "Epoch 1/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 1.4363 - categorical_accuracy: 0.49866263: batch_generator Function called.\n",
            "Size of all_data in batch_generator function: 328\n",
            "\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.42361, saving model to ./final/0-weights.01-0.4986-3.0434-0.4236.h5\n",
            "91/91 [==============================] - 75s 552ms/step - loss: 1.4363 - categorical_accuracy: 0.4986 - val_loss: 3.0434 - val_categorical_accuracy: 0.4236 - lr: 0.0050\n",
            "Epoch 2/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.9715 - categorical_accuracy: 0.5800\n",
            "Epoch 2: val_categorical_accuracy improved from 0.42361 to 0.48958, saving model to ./final/0-weights.02-0.5800-1.3530-0.4896.h5\n",
            "91/91 [==============================] - 31s 340ms/step - loss: 0.9715 - categorical_accuracy: 0.5800 - val_loss: 1.3530 - val_categorical_accuracy: 0.4896 - lr: 0.0050\n",
            "Epoch 3/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.8671 - categorical_accuracy: 0.6439\n",
            "Epoch 3: val_categorical_accuracy improved from 0.48958 to 0.56250, saving model to ./final/0-weights.03-0.6439-1.0178-0.5625.h5\n",
            "91/91 [==============================] - 31s 342ms/step - loss: 0.8671 - categorical_accuracy: 0.6439 - val_loss: 1.0178 - val_categorical_accuracy: 0.5625 - lr: 0.0050\n",
            "Epoch 4/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.7725 - categorical_accuracy: 0.6844\n",
            "Epoch 4: val_categorical_accuracy did not improve from 0.56250\n",
            "91/91 [==============================] - 30s 332ms/step - loss: 0.7725 - categorical_accuracy: 0.6844 - val_loss: 1.0247 - val_categorical_accuracy: 0.5590 - lr: 0.0050\n",
            "Epoch 5/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.6614 - categorical_accuracy: 0.7421\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.56250\n",
            "91/91 [==============================] - 30s 332ms/step - loss: 0.6614 - categorical_accuracy: 0.7421 - val_loss: 1.0967 - val_categorical_accuracy: 0.5521 - lr: 0.0050\n",
            "Epoch 6/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.5726 - categorical_accuracy: 0.7723\n",
            "Epoch 6: val_categorical_accuracy did not improve from 0.56250\n",
            "91/91 [==============================] - 31s 337ms/step - loss: 0.5726 - categorical_accuracy: 0.7723 - val_loss: 1.3143 - val_categorical_accuracy: 0.5000 - lr: 0.0050\n",
            "Epoch 7/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.5358 - categorical_accuracy: 0.7864\n",
            "Epoch 7: val_categorical_accuracy improved from 0.56250 to 0.62847, saving model to ./final/0-weights.07-0.7864-1.0272-0.6285.h5\n",
            "91/91 [==============================] - 31s 341ms/step - loss: 0.5358 - categorical_accuracy: 0.7864 - val_loss: 1.0272 - val_categorical_accuracy: 0.6285 - lr: 0.0050\n",
            "Epoch 8/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.4428 - categorical_accuracy: 0.8225\n",
            "Epoch 8: val_categorical_accuracy improved from 0.62847 to 0.66319, saving model to ./final/0-weights.08-0.8225-0.8769-0.6632.h5\n",
            "91/91 [==============================] - 31s 341ms/step - loss: 0.4428 - categorical_accuracy: 0.8225 - val_loss: 0.8769 - val_categorical_accuracy: 0.6632 - lr: 0.0050\n",
            "Epoch 9/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.3859 - categorical_accuracy: 0.8582\n",
            "Epoch 9: val_categorical_accuracy did not improve from 0.66319\n",
            "91/91 [==============================] - 30s 332ms/step - loss: 0.3859 - categorical_accuracy: 0.8582 - val_loss: 0.9845 - val_categorical_accuracy: 0.6111 - lr: 0.0050\n",
            "Epoch 10/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.3628 - categorical_accuracy: 0.8613\n",
            "Epoch 10: val_categorical_accuracy did not improve from 0.66319\n",
            "91/91 [==============================] - 31s 337ms/step - loss: 0.3628 - categorical_accuracy: 0.8613 - val_loss: 1.0257 - val_categorical_accuracy: 0.6562 - lr: 0.0050\n",
            "Epoch 11/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.2997 - categorical_accuracy: 0.8860\n",
            "Epoch 11: val_categorical_accuracy did not improve from 0.66319\n",
            "91/91 [==============================] - 31s 336ms/step - loss: 0.2997 - categorical_accuracy: 0.8860 - val_loss: 1.4579 - val_categorical_accuracy: 0.5868 - lr: 0.0050\n",
            "Epoch 12/12\n",
            "91/91 [==============================] - ETA: 0s - loss: 0.2656 - categorical_accuracy: 0.9049\n",
            "Epoch 12: val_categorical_accuracy did not improve from 0.66319\n",
            "91/91 [==============================] - 30s 332ms/step - loss: 0.2656 - categorical_accuracy: 0.9049 - val_loss: 1.0213 - val_categorical_accuracy: 0.6250 - lr: 0.0050\n",
            "<--------------------------------------------------Confusion Matrix process starts-------------------------------------------------->\n",
            "Batch size: 32\n",
            "38296: batch_generator_confusion_matrix Function called.\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "len(test_data_batch): 32\n",
            "Total_predicted_labels: 320\n",
            "----------------------------------Printing confusion matrix for the iteration - 0---------------------------------\n",
            "[[84 24 22  0]\n",
            " [32 19  9  1]\n",
            " [11  4 59  3]\n",
            " [ 3  1 17 31]]\n",
            "Iteration no: 1\n",
            "38297: slice_train_test Function called.\n",
            "Category-wise train-size: 1200, Category-wise test-size: 133\n",
            "38298: slice_train_test Function called.\n",
            "Category-wise train-size: 573, Category-wise test-size: 63\n",
            "38299: slice_train_test Function called.\n",
            "Category-wise train-size: 719, Category-wise test-size: 79\n",
            "38300: slice_train_test Function called.\n",
            "Category-wise train-size: 482, Category-wise test-size: 53\n",
            "Size of training dataset: 2974, no. of training labels: 2974\n",
            "Size of testing dataset: 328, no. of testing labels: 328\n",
            "41603: Network_Config Function called.\n",
            "41604: non_local_block Function called.\n",
            "K.image_data_format(): channels_last\n",
            "41605: _convND Function called.\n",
            "41606: _convND Function called.\n",
            "41607: _convND Function called.\n",
            "41608: _convND Function called.\n",
            "41609: squeeze_excitation_layer Function called.\n",
            "input_1\n",
            "block1_conv1\n",
            "block1_conv2\n",
            "block1_pool\n",
            "block2_conv1\n",
            "block2_conv2\n",
            "block2_pool\n",
            "block3_conv1\n",
            "block3_conv2\n",
            "block3_conv3\n",
            "block3_pool\n",
            "block4_conv1\n",
            "block4_conv2\n",
            "block4_conv3\n",
            "block4_pool\n",
            "block5_conv1\n",
            "block5_conv2\n",
            "block5_conv3\n",
            "block5_pool\n",
            "conv2d_1\n",
            "conv2d\n",
            "reshape_1\n",
            "reshape\n",
            "max_pooling1d\n",
            "conv2d_2\n",
            "global_average_pooling2d\n",
            "dot\n",
            "reshape_2\n",
            "dense\n",
            "activation\n",
            "max_pooling1d_1\n",
            "activation_1\n",
            "dot_1\n",
            "dense_1\n",
            "reshape_3\n",
            "activation_2\n",
            "conv2d_3\n",
            "reshape_4\n",
            "batch_normalization\n",
            "multiply\n",
            "concatenate\n",
            "batch_normalization_1\n",
            "concatenate_1\n",
            "global_average_pooling2d_1\n",
            "flatten\n",
            "concatenate_2\n",
            "dense_2\n",
            "batch_normalization_2\n",
            "dense_3\n",
            "batch_normalization_3\n",
            "dense_4\n",
            "batch_normalization_4\n",
            "dense_5\n",
            "<--------------------------------------------------Model Summary--------------------------------------------------------->\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block5_conv2 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv3 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block5_pool (MaxPooling2D)     (None, 7, 7, 512)    0           ['block5_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 7, 7, 256)    131072      ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 7, 7, 256)    131072      ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 49, 256)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 49, 256)      0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 24, 256)      0           ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 7, 7, 256)    131072      ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 512)         0           ['block5_pool[0][0]']            \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 49, 24)       0           ['reshape[0][0]',                \n",
            "                                                                  'max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 49, 256)      0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          65664       ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 49, 24)       0           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 24, 256)     0           ['reshape_2[0][0]']              \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dot_1 (Dot)                    (None, 49, 256)      0           ['activation[0][0]',             \n",
            "                                                                  'max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 512)          66048       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)            (None, 7, 7, 256)    0           ['dot_1[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 512)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 7, 7, 512)    131072      ['reshape_3[0][0]']              \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)            (None, 1, 1, 512)    0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 7, 7, 512)   2048        ['conv2d_3[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 7, 7, 512)    0           ['block5_pool[0][0]',            \n",
            "                                                                  'reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 7, 7, 1024)   0           ['block5_pool[0][0]',            \n",
            "                                                                  'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 7, 7, 512)   2048        ['multiply[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 7, 7, 1536)   0           ['concatenate[0][0]',            \n",
            "                                                                  'batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 1536)        0           ['concatenate_1[0][0]']          \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 75264)        0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 76800)        0           ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 512)          39322112    ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 512)         2048        ['dense_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 512)          262656      ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 512)         2048        ['dense_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 512)          262656      ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 512)         2048        ['dense_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 4)            2052        ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 55,230,404\n",
            "Trainable params: 40,510,596\n",
            "Non-trainable params: 14,719,808\n",
            "__________________________________________________________________________________________________\n",
            "------------------------------------------------End of model summary------------------------------------------------------\n",
            "41610: create_directory Function called.\n",
            "./final/ is created using create_directory Function.\n",
            "Printing test label: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    global Step_no\n",
        "    print(str(Step_no) + \": main Function called.\")\n",
        "    Step_no += 1\n",
        "    list = creat_list(\"file_list.txt\")\n",
        "    cross_validation(categorised_data = list, K = 10, no_of_epochs = 12, no_of_classes = 4, batch_size = 32)\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TJ3OM2UqDh-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33251969-b4cf-4362-9f1b-7a3babd519ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `jupyter nbconvert --to html /content/EC(M)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qDDEIuZ5o-Dj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}